{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98abd45-8c08-484b-b4cb-15fa8bff1cbb",
      "metadata": {
        "tags": [],
        "id": "b98abd45-8c08-484b-b4cb-15fa8bff1cbb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import jieba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec3a5a9-4e63-4410-8576-d9309f55e17d",
      "metadata": {
        "tags": [],
        "id": "7ec3a5a9-4e63-4410-8576-d9309f55e17d"
      },
      "outputs": [],
      "source": [
        "#n-gram tokenizer without spaces\n",
        "def n_gram_tokenizer(text, n = 1):\n",
        "    return [text[i:i+n] for i in range(len(text) - (n - 1))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a078d31-e755-477d-8562-2b2c7fbf0c01",
      "metadata": {
        "tags": [],
        "id": "3a078d31-e755-477d-8562-2b2c7fbf0c01"
      },
      "outputs": [],
      "source": [
        "texts = []\n",
        "\n",
        "with open('texts.txt', 'r') as f:\n",
        "    for i, line in enumerate(f, 1):\n",
        "        text = line.strip()  # Remove leading/trailing whitespace and newline characters\n",
        "        if not text:\n",
        "            print(f\"Warning: Empty text found on line {i}.\")\n",
        "        texts.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c775bd0e-8350-49c4-9640-87e89f699803",
      "metadata": {
        "tags": [],
        "id": "c775bd0e-8350-49c4-9640-87e89f699803"
      },
      "outputs": [],
      "source": [
        "genres = []\n",
        "\n",
        "with open('genres.txt', 'r') as f:\n",
        "    for i, line in enumerate(f, 1):\n",
        "        genre = line.strip()  # Remove leading/trailing whitespace and newline characters\n",
        "        if not text:\n",
        "            print(f\"Warning: Empty text found on line {i}.\")\n",
        "        genres.append(genre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e11eed6-52aa-4f4c-9eef-91164d17db16",
      "metadata": {
        "id": "7e11eed6-52aa-4f4c-9eef-91164d17db16"
      },
      "outputs": [],
      "source": [
        "def two_gram_tokenizer(text):\n",
        "    return n_gram_tokenizer(text, n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aded3459-e7b2-44a4-a347-2d44c5cd48f7",
      "metadata": {
        "tags": [],
        "id": "aded3459-e7b2-44a4-a347-2d44c5cd48f7"
      },
      "outputs": [],
      "source": [
        "chunk_size = 1000\n",
        "\n",
        "vectorizer_2gram = TfidfVectorizer(max_features=100, analyzer='word', tokenizer=two_gram_tokenizer)\n",
        "\n",
        "# Tokenize and transform texts in chunks\n",
        "all_chunk_texts = []\n",
        "all_chunk_genres = []\n",
        "for i, text in enumerate(texts): # for text in texts:\n",
        "    chunk_texts = [text[j:j + chunk_size] for j in range(0, len(text), chunk_size)]\n",
        "    chunk_genres = [genres[i] for _ in range(len(chunk_texts))]  # Assign the corresponding genre to each chunk\n",
        "    all_chunk_texts.extend(chunk_texts)\n",
        "    all_chunk_genres.extend(chunk_genres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb6d5344-2462-4f44-9656-0ac60d280b91",
      "metadata": {
        "tags": [],
        "id": "bb6d5344-2462-4f44-9656-0ac60d280b91"
      },
      "outputs": [],
      "source": [
        "def jieba_tokenizer(text):\n",
        "    return ''.join(jieba.lcut(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78c0ba34-49a5-4962-8e76-b9509a07c993",
      "metadata": {
        "tags": [],
        "id": "78c0ba34-49a5-4962-8e76-b9509a07c993"
      },
      "outputs": [],
      "source": [
        "chunk_size = 1000\n",
        "\n",
        "jieba_chunk_texts = []\n",
        "jieba_chunk_genres = []\n",
        "for i, text in enumerate(texts):\n",
        "    chunk_tokens = jieba.lcut(text)  # Tokenize text using Jieba\n",
        "    chunk_texts = [''.join(chunk_tokens[j:j + chunk_size]) for j in range(0, len(chunk_tokens), chunk_size)]\n",
        "    chunk_genres = [genres[i] for _ in range(len(chunk_texts))]  # Assign the corresponding genre to each chunk\n",
        "    jieba_chunk_texts.extend(chunk_texts)\n",
        "    jieba_chunk_genres.extend(chunk_genres)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18d633ea-d806-416c-aa56-d1585e4f5127",
      "metadata": {
        "id": "18d633ea-d806-416c-aa56-d1585e4f5127"
      },
      "source": [
        "1 gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c003b55-53e1-4df7-b5a2-a51bc40a65d6",
      "metadata": {
        "tags": [],
        "id": "3c003b55-53e1-4df7-b5a2-a51bc40a65d6"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=100, tokenizer=n_gram_tokenizer)\n",
        "vectors = vectorizer.fit_transform(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c287e69-01f8-4df8-9ad4-de7f4a9f4045",
      "metadata": {
        "tags": [],
        "id": "1c287e69-01f8-4df8-9ad4-de7f4a9f4045"
      },
      "outputs": [],
      "source": [
        "X = vectors\n",
        "y = genres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d310bb-f0df-4922-b0e7-9724eee6963a",
      "metadata": {
        "tags": [],
        "id": "a8d310bb-f0df-4922-b0e7-9724eee6963a"
      },
      "outputs": [],
      "source": [
        "clf = SGDClassifier(loss = 'hinge', max_iter = 5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311bc2c6-80ca-4759-b2b3-3f7cb7afefa3",
      "metadata": {
        "tags": [],
        "id": "311bc2c6-80ca-4759-b2b3-3f7cb7afefa3",
        "outputId": "4bc9df46-026b-41e3-ec86-044f41bdc582"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/lola/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.79777236 0.80403759 0.81691612 0.81726418 0.80090498 0.80682214\n",
            " 0.80536212 0.79735376 0.80571031 0.80118384]\n"
          ]
        }
      ],
      "source": [
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c2e19a5-dc87-41ea-8c6d-714d78c2f14e",
      "metadata": {
        "id": "2c2e19a5-dc87-41ea-8c6d-714d78c2f14e"
      },
      "source": [
        "2 gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1893add8-ca1b-4102-a396-8a4234f7e346",
      "metadata": {
        "tags": [],
        "id": "1893add8-ca1b-4102-a396-8a4234f7e346"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load data from the .p file\n",
        "with open('2gramvectorizer.p', 'rb') as file:\n",
        "    twogramvectorizer = pickle.load(file)\n",
        "\n",
        "with open('2gramchunks.p', 'rb') as file:\n",
        "    twogramchunks = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "457d13d6-e4a7-4c8e-be1e-e30b4503d424",
      "metadata": {
        "tags": [],
        "id": "457d13d6-e4a7-4c8e-be1e-e30b4503d424"
      },
      "outputs": [],
      "source": [
        "X = twogramchunks\n",
        "y = np.array(all_chunk_genres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b3c004-17d6-449e-bf6d-d68640757352",
      "metadata": {
        "tags": [],
        "id": "33b3c004-17d6-449e-bf6d-d68640757352",
        "outputId": "d8221828-aad6-4cf6-bc23-2ea2bc291094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.65466096 0.70412623 0.72173324 0.68237112 0.65623487 0.6785302\n",
            " 0.67306272 0.7002818  0.68376464 0.71413513]\n"
          ]
        }
      ],
      "source": [
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7dcf531-372a-4421-b5cb-0e03a3e48306",
      "metadata": {
        "id": "c7dcf531-372a-4421-b5cb-0e03a3e48306"
      },
      "source": [
        "Jieba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbfa1440-35da-495f-af14-d8e8721bd416",
      "metadata": {
        "tags": [],
        "id": "fbfa1440-35da-495f-af14-d8e8721bd416"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f992181-a1a4-4d78-93a0-d195eeb018b0",
      "metadata": {
        "tags": [],
        "id": "6f992181-a1a4-4d78-93a0-d195eeb018b0"
      },
      "outputs": [],
      "source": [
        "import jieba\n",
        "\n",
        "# Load data from the .p file\n",
        "with open('jbvectorizer.p', 'rb') as file:\n",
        "    jbvectorizer = pickle.load(file)\n",
        "\n",
        "with open('jbchunks.p', 'rb') as file:\n",
        "    jbchunks = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a1f5d6-4732-48be-a83b-b8165af65781",
      "metadata": {
        "tags": [],
        "id": "06a1f5d6-4732-48be-a83b-b8165af65781"
      },
      "outputs": [],
      "source": [
        "X = jbchunks\n",
        "y = np.array(all_chunk_genres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af5ae3ff-e7fd-4aa3-a412-c6489d89c5e0",
      "metadata": {
        "tags": [],
        "id": "af5ae3ff-e7fd-4aa3-a412-c6489d89c5e0",
        "outputId": "92e65a17-68f5-42de-d95e-27d785932fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.71315022 0.73677682 0.77117341 0.71547703 0.69327736 0.70838084\n",
            " 0.69900904 0.7408236  0.71775622 0.74930267]\n"
          ]
        }
      ],
      "source": [
        "scores = cross_val_score(clf, X, y, cv=10)\n",
        "print(scores)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}