{"cells":[{"cell_type":"code","execution_count":null,"id":"b98abd45-8c08-484b-b4cb-15fa8bff1cbb","metadata":{"tags":[],"id":"b98abd45-8c08-484b-b4cb-15fa8bff1cbb"},"outputs":[],"source":["import pandas as pd\n","import os\n","import re\n","import csv\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","import plotly.express as px\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.model_selection import cross_val_score\n","import jieba"]},{"cell_type":"code","execution_count":null,"id":"7ec3a5a9-4e63-4410-8576-d9309f55e17d","metadata":{"tags":[],"id":"7ec3a5a9-4e63-4410-8576-d9309f55e17d"},"outputs":[],"source":["#n-gram tokenizer without spaces\n","def n_gram_tokenizer(text, n = 1):\n","    return [text[i:i+n] for i in range(len(text) - (n - 1))]\n","\n","#did we want this to be like this? ['我 对', '对 中', '中 文', '文 感', '感 兴', '兴 趣'], or all in one string"]},{"cell_type":"code","execution_count":null,"id":"3a078d31-e755-477d-8562-2b2c7fbf0c01","metadata":{"tags":[],"id":"3a078d31-e755-477d-8562-2b2c7fbf0c01"},"outputs":[],"source":["texts = []\n","\n","with open('texts.txt', 'r') as f:\n","    for i, line in enumerate(f, 1):\n","        text = line.strip()  # Remove leading/trailing whitespace and newline characters\n","        if not text:\n","            print(f\"Warning: Empty text found on line {i}.\")\n","        texts.append(text)"]},{"cell_type":"code","execution_count":null,"id":"c775bd0e-8350-49c4-9640-87e89f699803","metadata":{"tags":[],"id":"c775bd0e-8350-49c4-9640-87e89f699803"},"outputs":[],"source":["genres = []\n","\n","with open('genres.txt', 'r') as f:\n","    for i, line in enumerate(f, 1):\n","        genre = line.strip()  # Remove leading/trailing whitespace and newline characters\n","        if not text:\n","            print(f\"Warning: Empty text found on line {i}.\")\n","        genres.append(genre)"]},{"cell_type":"code","execution_count":null,"id":"7e11eed6-52aa-4f4c-9eef-91164d17db16","metadata":{"id":"7e11eed6-52aa-4f4c-9eef-91164d17db16"},"outputs":[],"source":["def two_gram_tokenizer(text):\n","    return n_gram_tokenizer(text, n=2)"]},{"cell_type":"code","execution_count":null,"id":"aded3459-e7b2-44a4-a347-2d44c5cd48f7","metadata":{"tags":[],"id":"aded3459-e7b2-44a4-a347-2d44c5cd48f7"},"outputs":[],"source":["chunk_size = 1000\n","\n","vectorizer_2gram = TfidfVectorizer(max_features=100, analyzer='word', tokenizer=two_gram_tokenizer)\n","\n","# Tokenize and transform texts in chunks\n","# PV: here is my proposed ammended code\n","all_chunk_texts = []\n","all_chunk_genres = []\n","for i, text in enumerate(texts): # for text in texts:\n","    chunk_texts = [text[j:j + chunk_size] for j in range(0, len(text), chunk_size)]\n","    chunk_genres = [genres[i] for _ in range(len(chunk_texts))]  # Assign the corresponding genre to each chunk\n","    all_chunk_texts.extend(chunk_texts)\n","    all_chunk_genres.extend(chunk_genres)"]},{"cell_type":"code","execution_count":null,"id":"bb6d5344-2462-4f44-9656-0ac60d280b91","metadata":{"tags":[],"id":"bb6d5344-2462-4f44-9656-0ac60d280b91"},"outputs":[],"source":["def jieba_tokenizer(text):\n","    return ''.join(jieba.lcut(text))"]},{"cell_type":"code","execution_count":null,"id":"78c0ba34-49a5-4962-8e76-b9509a07c993","metadata":{"tags":[],"id":"78c0ba34-49a5-4962-8e76-b9509a07c993","outputId":"592cfa32-0787-4cbf-d3f8-76e1b66bfb71"},"outputs":[{"name":"stderr","output_type":"stream","text":["Building prefix dict from the default dictionary ...\n","Loading model from cache /var/folders/2r/fj2_zqjj6mj4cldvkdp5_bx00000gs/T/jieba.cache\n","Loading model cost 0.568 seconds.\n","Prefix dict has been built successfully.\n"]}],"source":["chunk_size = 1000\n","\n","jieba_chunk_texts = []\n","jieba_chunk_genres = []\n","for i, text in enumerate(texts):\n","    chunk_tokens = jieba.lcut(text)  # Tokenize text using Jieba\n","    chunk_texts = [''.join(chunk_tokens[j:j + chunk_size]) for j in range(0, len(chunk_tokens), chunk_size)]\n","    chunk_genres = [genres[i] for _ in range(len(chunk_texts))]  # Assign the corresponding genre to each chunk\n","    jieba_chunk_texts.extend(chunk_texts)\n","    jieba_chunk_genres.extend(chunk_genres)"]},{"cell_type":"markdown","id":"18d633ea-d806-416c-aa56-d1585e4f5127","metadata":{"id":"18d633ea-d806-416c-aa56-d1585e4f5127"},"source":["1 gram"]},{"cell_type":"code","execution_count":null,"id":"3c003b55-53e1-4df7-b5a2-a51bc40a65d6","metadata":{"tags":[],"id":"3c003b55-53e1-4df7-b5a2-a51bc40a65d6","outputId":"d1b39b40-e7d0-4c8d-c96d-fbe3328560b5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/lola/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, tokenizer\u001b[38;5;241m=\u001b[39mn_gram_tokenizer)\n\u001b[0;32m----> 2\u001b[0m vectors \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(texts)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2126\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2121\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2122\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2123\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2124\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2125\u001b[0m )\n\u001b[0;32m-> 2126\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2129\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1383\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1376\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1377\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1378\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1379\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1380\u001b[0m             )\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1383\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1386\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1270\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1269\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1272\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n","File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:112\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    110\u001b[0m     doc \u001b[38;5;241m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ngrams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_words \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36mn_gram_tokenizer\u001b[0;34m(text, n)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mn_gram_tokenizer\u001b[39m(text, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[i:i\u001b[38;5;241m+\u001b[39mn] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m-\u001b[39m (n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))]\n","Cell \u001b[0;32mIn[2], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mn_gram_tokenizer\u001b[39m(text, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[i:i\u001b[38;5;241m+\u001b[39mn] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m-\u001b[39m (n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["vectorizer = TfidfVectorizer(max_features=100, tokenizer=n_gram_tokenizer)\n","vectors = vectorizer.fit_transform(texts)"]},{"cell_type":"code","execution_count":null,"id":"1c287e69-01f8-4df8-9ad4-de7f4a9f4045","metadata":{"tags":[],"id":"1c287e69-01f8-4df8-9ad4-de7f4a9f4045"},"outputs":[],"source":["X = vectors\n","y = genres"]},{"cell_type":"code","execution_count":null,"id":"a8d310bb-f0df-4922-b0e7-9724eee6963a","metadata":{"tags":[],"id":"a8d310bb-f0df-4922-b0e7-9724eee6963a"},"outputs":[],"source":["clf = SGDClassifier(loss = 'hinge', max_iter = 5000)"]},{"cell_type":"code","execution_count":null,"id":"311bc2c6-80ca-4759-b2b3-3f7cb7afefa3","metadata":{"tags":[],"id":"311bc2c6-80ca-4759-b2b3-3f7cb7afefa3","outputId":"4bc9df46-026b-41e3-ec86-044f41bdc582"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/lola/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[0.79777236 0.80403759 0.81691612 0.81726418 0.80090498 0.80682214\n"," 0.80536212 0.79735376 0.80571031 0.80118384]\n"]}],"source":["scores = cross_val_score(clf, X, y, cv=10)\n","print(scores)"]},{"cell_type":"markdown","id":"2c2e19a5-dc87-41ea-8c6d-714d78c2f14e","metadata":{"id":"2c2e19a5-dc87-41ea-8c6d-714d78c2f14e"},"source":["2 gram"]},{"cell_type":"code","execution_count":null,"id":"1893add8-ca1b-4102-a396-8a4234f7e346","metadata":{"tags":[],"id":"1893add8-ca1b-4102-a396-8a4234f7e346","outputId":"7cca68ba-a98f-4805-ee99-d005eecfb090"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/2r/fj2_zqjj6mj4cldvkdp5_bx00000gs/T/ipykernel_52238/1010760311.py:8: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n","  twogramchunks = pickle.load(file)\n"]}],"source":["import pickle\n","\n","# Load data from the .p file\n","with open('2gramvectorizer.p', 'rb') as file:\n","    twogramvectorizer = pickle.load(file)\n","\n","with open('2gramchunks.p', 'rb') as file:\n","    twogramchunks = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"id":"457d13d6-e4a7-4c8e-be1e-e30b4503d424","metadata":{"tags":[],"id":"457d13d6-e4a7-4c8e-be1e-e30b4503d424"},"outputs":[],"source":["X = twogramchunks\n","y = np.array(all_chunk_genres)"]},{"cell_type":"code","execution_count":null,"id":"33b3c004-17d6-449e-bf6d-d68640757352","metadata":{"tags":[],"id":"33b3c004-17d6-449e-bf6d-d68640757352","outputId":"d8221828-aad6-4cf6-bc23-2ea2bc291094"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.65466096 0.70412623 0.72173324 0.68237112 0.65623487 0.6785302\n"," 0.67306272 0.7002818  0.68376464 0.71413513]\n"]}],"source":["scores = cross_val_score(clf, X, y, cv=10)\n","print(scores)"]},{"cell_type":"markdown","id":"c7dcf531-372a-4421-b5cb-0e03a3e48306","metadata":{"id":"c7dcf531-372a-4421-b5cb-0e03a3e48306"},"source":["Jieba"]},{"cell_type":"code","execution_count":null,"id":"fbfa1440-35da-495f-af14-d8e8721bd416","metadata":{"tags":[],"id":"fbfa1440-35da-495f-af14-d8e8721bd416"},"outputs":[],"source":["import pickle"]},{"cell_type":"code","execution_count":null,"id":"6f992181-a1a4-4d78-93a0-d195eeb018b0","metadata":{"tags":[],"id":"6f992181-a1a4-4d78-93a0-d195eeb018b0","outputId":"17fb7ef4-08e4-4560-ae5c-834bd584a874"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/2r/fj2_zqjj6mj4cldvkdp5_bx00000gs/T/ipykernel_52238/4150266919.py:8: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n","  jbvectorizer = pickle.load(file)\n","/Users/lola/anaconda3/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.0.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/Users/lola/anaconda3/lib/python3.11/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.0.2 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/var/folders/2r/fj2_zqjj6mj4cldvkdp5_bx00000gs/T/ipykernel_52238/4150266919.py:11: DeprecationWarning: Please use `csr_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.csr` namespace is deprecated.\n","  jbchunks = pickle.load(file)\n"]}],"source":["import jieba\n","\n","# Load data from the .p file\n","with open('jbvectorizer.p', 'rb') as file:\n","    jbvectorizer = pickle.load(file)\n","\n","with open('jbchunks.p', 'rb') as file:\n","    jbchunks = pickle.load(file)"]},{"cell_type":"code","execution_count":null,"id":"06a1f5d6-4732-48be-a83b-b8165af65781","metadata":{"tags":[],"id":"06a1f5d6-4732-48be-a83b-b8165af65781"},"outputs":[],"source":["X = jbchunks\n","y = np.array(all_chunk_genres)"]},{"cell_type":"code","execution_count":null,"id":"af5ae3ff-e7fd-4aa3-a412-c6489d89c5e0","metadata":{"tags":[],"id":"af5ae3ff-e7fd-4aa3-a412-c6489d89c5e0","outputId":"92e65a17-68f5-42de-d95e-27d785932fab"},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.71315022 0.73677682 0.77117341 0.71547703 0.69327736 0.70838084\n"," 0.69900904 0.7408236  0.71775622 0.74930267]\n"]}],"source":["scores = cross_val_score(clf, X, y, cv=10)\n","print(scores)"]},{"cell_type":"code","execution_count":null,"id":"071d31cd-31ac-4540-b6d9-550fe73c41a2","metadata":{"id":"071d31cd-31ac-4540-b6d9-550fe73c41a2"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}